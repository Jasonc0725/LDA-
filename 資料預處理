import os
import re
import numpy as np
import pandas as pd
from collections import Counter
import matplotlib.pyplot as plt

import warnings
warnings.filterwarnings("ignore")
from ckip_transformers.nlp import CkipWordSegmenter, CkipPosTagger, CkipNerChunker

# Initialize drivers
print("Initializing drivers ... WS")
ws_driver = CkipWordSegmenter(model="bert-base", device=-1)
print("Initializing drivers ... POS")
pos_driver = CkipPosTagger(model="bert-base", device=-1)
print("Initializing drivers ... NER")
ner_driver = CkipNerChunker(model="bert-base", device=-1)
print("Initializing drivers ... all done")

# 打印當前工作目錄
print("Current working directory:", os.getcwd())

# 加載 CSV 文件
ORIGINAL_DATA = pd.read_csv("./df_news.csv")

# 檢查列名
print("Columns before dropping:", ORIGINAL_DATA.columns)

# 刪除 'Unnamed: 0' 列（如果存在）
if 'Unnamed: 0' in ORIGINAL_DATA.columns:
    ORIGINAL_DATA = ORIGINAL_DATA.drop(columns=["Unnamed: 0"])

# 檢查列名
print("Columns after dropping:", ORIGINAL_DATA.columns)

# 打印數據框的行數
print(f"Total number of rows in ORIGINAL_DATA: {len(ORIGINAL_DATA)}")

# 確保樣本數量不超過總行數
sample_size = min(500, len(ORIGINAL_DATA))

# 隨機抽取樣本數據
TEST_DATA = ORIGINAL_DATA.sample(n=sample_size, random_state=42)

# 提取 'content' 列
content = TEST_DATA["內容"]

# 顯示 'content' 列的前10條數據
print(content.head(10))

# 匯入停用詞
def read_stopword():
    with open("./stopwords_TW.txt", "r", encoding="utf-8") as f:
        stopword = [word.strip("\n") for word in f.readlines()]
    return stopword

stopwords = read_stopword()

# 對文章進行斷詞
def do_CKIP_WS(article):
    ws_results = ws_driver([str(article)])
    return ws_results

# 對詞組進行詞性標示
def do_CKIP_POS(ws_result):
    pos = pos_driver(ws_result[0])
    all_list = []
    for sent in pos:
        all_list.append(sent)
    return all_list

# 保留名詞與動詞
def pos_filter(pos):
    for i in list(set(pos)):
        if i.startswith("N") or i.startswith("V"):
            return "Yes"
        else:
            continue

# 去除數字與網址詞組
def remove_number_url(ws):
    number_pattern = "^\d+\.?\d*"
    url_pattern = "^https?:\\/\\/(?:www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b(?:[-a-zA-Z0-9()@:%_\\+.~#?&\\/=]*)$"
    space_pattern = "^ .*"
    num_regex = re.match(number_pattern, ws)
    url_regex = re.match(url_pattern, ws)
    space_regex = re.match(space_pattern, ws)
    if num_regex ==  None and url_regex == None and space_regex == None:
        return True
    else:
        return False

# 執行資料清洗
def cleaner(ws_results, pos_results, stopwords):
    word_lst = []
    for ws, pos in zip(ws_results[0], pos_results):
        in_stopwords_or_not = ws not in stopwords  #詞組是否存為停用詞
        if_len_greater_than_1 = len(ws) > 1        #詞組長度必須大於1
        is_V_or_N = pos_filter(pos)                #詞組是否為名詞、動詞
        is_num_or_url = remove_number_url(ws)      #詞組是否為數字、網址、空白開頭
        if in_stopwords_or_not and if_len_greater_than_1 and is_V_or_N == "Yes" and is_num_or_url:
            word_lst.append(str(ws))
        else:
            pass
    return word_lst
seg_lst = []
for i in range(len(TEST_DATA)):
    ws_results = do_CKIP_WS(content.iloc[i])
    pos_results = do_CKIP_POS(ws_results)
    word_lst = cleaner(ws_results, pos_results, stopwords)
    seg_lst.append(word_lst)
